{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCpSJJfut4VrBbT1n8ggt6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Importing tools**"
      ],
      "metadata": {
        "id": "8tQ3kMwrkwN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy import signal"
      ],
      "metadata": {
        "id": "b8JBcSLfkz_Z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3oey6YDlBOH",
        "outputId": "ad3e0b16-1fc4-47e0-95a8-c7e905637c79"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Layer class**"
      ],
      "metadata": {
        "id": "veQ07UVDEwDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer():\n",
        "  def __init__(self):\n",
        "    self.input = None\n",
        "    self.output = None\n",
        "  def forward_prop(self, input):\n",
        "    pass\n",
        "  def backward_prop(self, output_gradient, learning_rate):\n",
        "    pass"
      ],
      "metadata": {
        "id": "EbaRcNulEyuf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Convolution class for convolutional layers**"
      ],
      "metadata": {
        "id": "CHgAnswz1dF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Convolution(Layer):\n",
        "  def __init__(self, input_shape, kernel_size, depth):        #depth reprezents the number of \"neurons\" in the layer\n",
        "    input_depth, input_height, input_width = input_shape\n",
        "    self.input_shape = input_shape\n",
        "    self.depth = depth\n",
        "    self.input_depth = input_depth\n",
        "    self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n",
        "    self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n",
        "    self.kernels = np.random.rand(*self.kernels_shape) - 0.5\n",
        "    self.biases = np.random.rand(*self.output_shape) - 0.5\n",
        "  def forward_prop(self, input):\n",
        "    self.input = input\n",
        "    self.output = np.copy(self.biases)\n",
        "    for i in range(self.depth):\n",
        "      for j in range(self.input_depth):\n",
        "        self.output[i] += signal.correlate2d(self.input[j], self.kernels[i,j], \"valid\")\n",
        "\n",
        "    return self.output\n",
        "  def backward_prop(self, output_gradient, learning_rate):\n",
        "    kernels_gradient = np.zeros(self.kernels_shape)\n",
        "    input_gradient = np.zeros(self.input_shape)\n",
        "    for i in range(self.depth):\n",
        "      for j in range(self.input_depth):\n",
        "        kernels_gradient[i,j] = signal.correlate2d(self.input[j], output_gradient[i], \"valid\")\n",
        "        input_gradient[j] += signal.convolve2d(output_gradient[i], self.kernels[i,j], \"full\")\n",
        "\n",
        "\n",
        "    self.kernels -= learning_rate * kernels_gradient\n",
        "    self.biases -= learning_rate * output_gradient\n",
        "    return input_gradient"
      ],
      "metadata": {
        "id": "x4OprYUG1cK9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Max pooling class**"
      ],
      "metadata": {
        "id": "lU591JeBawVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool2D(Layer):\n",
        "  def __init__(self, size = 2, stride = 2):\n",
        "    self.size = size\n",
        "    self.stride = stride\n",
        "    self.original = None\n",
        "  def forward_prop(self, input):\n",
        "    C,H,W = input.shape\n",
        "\n",
        "    S = self.stride\n",
        "    self.input = input\n",
        "    HH = round(1 + (H - self.size)/S)\n",
        "    WW = round(1 + (W - self.size)/S)\n",
        "    self.output = np.zeros((C,HH,WW))\n",
        "\n",
        "    for depth in range(C):\n",
        "        for r in range(0,H,S):\n",
        "          for c in range(0,W,S):\n",
        "            self.output[depth, int(r/S), int(c/S)] = np.max(self.input[depth, r:r+self.size, c:c+self.size])\n",
        "\n",
        "    return self.output\n",
        "  def backward_prop(self, output_gradient, learning_rate):\n",
        "    C,H,W = self.input.shape\n",
        "    S = self.stride\n",
        "    C,HH,WW = output_gradient.shape\n",
        "\n",
        "    input_gradient = np.zeros(self.input.shape)\n",
        "\n",
        "    for depth in range(C):\n",
        "        for r in range(HH):\n",
        "          for c in range(WW):\n",
        "            x_pool = self.input[depth, r*S:r*S+self.size, c*S:c*S+self.size]\n",
        "            mask = (x_pool == np.max(x_pool))\n",
        "            input_gradient[depth, r*S:r*S+self.size, c*S:c*S+self.size] = mask*output_gradient[depth,r,c]\n",
        "\n",
        "    return input_gradient\n",
        "\n"
      ],
      "metadata": {
        "id": "hl_8_n3Vavu8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Reshape class**"
      ],
      "metadata": {
        "id": "N_pprMUNdSO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Reshape(Layer):\n",
        "    def __init__(self, input_shape, output_shape):\n",
        "      self.input_shape = input_shape\n",
        "      self.output_shape = output_shape\n",
        "\n",
        "    def forward_prop(self, input):\n",
        "      return np.reshape(input, self.output_shape)\n",
        "\n",
        "    def backward_prop(self, output_gradient, learning_rate):\n",
        "      return np.reshape(output_gradient, self.input_shape)"
      ],
      "metadata": {
        "id": "uV-6J_z2dVl1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Dense class for normal hidden layers**"
      ],
      "metadata": {
        "id": "vaVnMUKWFAW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dense(Layer):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    self.weights = np.random.rand(output_size, input_size) - 0.5\n",
        "    self.bias = np.random.rand(output_size, 1) -0.5\n",
        "  def forward_prop(self, input):\n",
        "    self.input = input\n",
        "    return np.dot(self.weights, self.input) + self.bias\n",
        "  def backward_prop(self, output_gradient, learning_rate):\n",
        "    weights_gradient = np.dot(output_gradient, self.input.T)\n",
        "    input_gradient = np.dot(self.weights.T, output_gradient)\n",
        "    self.weights -= learning_rate * weights_gradient\n",
        "    self.bias -= learning_rate * output_gradient\n",
        "    return input_gradient"
      ],
      "metadata": {
        "id": "2aoDDuPfmjjd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Activation class**"
      ],
      "metadata": {
        "id": "jS7gKtEhNiBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation(Layer):\n",
        "  def __init__(self, activation, activation_prime):\n",
        "    self.activation = activation\n",
        "    self.activation_prime = activation_prime\n",
        "  def forward_prop(self, input):\n",
        "    self.input = input\n",
        "    return self.activation(self.input)\n",
        "  def backward_prop(self, output_gradient, learning_rate):\n",
        "    return np.multiply(output_gradient, self.activation_prime(self.input))"
      ],
      "metadata": {
        "id": "dlc1RFp5PUV-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ReLU and Softmax classes**"
      ],
      "metadata": {
        "id": "OaF3HbvcRAQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU(Activation):\n",
        "  def __init__(self):\n",
        "    def relu(x):\n",
        "      return np.maximum(x, 0)\n",
        "    def relu_prime(x):\n",
        "      return x > 0\n",
        "    super().__init__(relu, relu_prime)\n",
        "class Softmax(Layer):\n",
        "  def forward_prop(self, input):\n",
        "    tmp = input - max(input)\n",
        "    x = np.exp(tmp)\n",
        "    self.output = x / sum(x)\n",
        "    return self.output\n",
        "  def backward_prop(self, output_gradient, learning_rate):\n",
        "    n = np.size(self.output)\n",
        "    return np.dot((np.identity(n) - self.output.T) * self.output, output_gradient)"
      ],
      "metadata": {
        "id": "VW6_sBv5RX4B"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Loss, Train, Predict functions**"
      ],
      "metadata": {
        "id": "zuoLkvAGSw_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(A):\n",
        "  return np.argmax(A, 0)\n",
        "\n",
        "def get_accuracy(Y_pred, Y_real):\n",
        "  return np.sum(Y_pred == Y_real) / Y_real.size\n",
        "\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "def binary_cross_entropy_prime(y_true, y_pred):\n",
        "    return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)\n",
        "\n",
        "def predict(network, input):\n",
        "    output = input\n",
        "    for layer in network:\n",
        "      output = layer.forward_prop(output)\n",
        "    return output\n",
        "\n",
        "def train(network, loss, loss_prime, x_train, y_train, epochs = 1000, learning_rate = 0.01, verbose = True):\n",
        "  for e in range(epochs):\n",
        "    error = 0\n",
        "    accuracy = 0\n",
        "    for x,y in zip(x_train, y_train):\n",
        "      output = predict(network, x)\n",
        "      error += loss(y,output)\n",
        "      if get_prediction(output) == get_prediction(y):\n",
        "        accuracy += 1\n",
        "      grad = loss_prime(y, output)\n",
        "      for layer in reversed(network):\n",
        "        grad = layer.backward_prop(grad, learning_rate)\n",
        "    error /= len(x_train)\n",
        "    accuracy /= len(x_train)\n",
        "    if verbose:\n",
        "      print(f\"{e + 1}/{epochs}, error={error}, accuracy={accuracy}\")"
      ],
      "metadata": {
        "id": "aCHTeZmbThej"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Training the CNN**"
      ],
      "metadata": {
        "id": "_PWjrdotUj7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/kaggle/MNIST_Dataset/train.csv\")\n",
        "data = np.array(data)\n",
        "np.random.shuffle(data)\n",
        "\n",
        "def preprocess_data(data_array):\n",
        "\n",
        "  data_train = data_array[100:1000,:]\n",
        "  Y_train = data_train[:,0]\n",
        "  Y_train = to_categorical(Y_train)\n",
        "  Y_train = Y_train.reshape(len(Y_train), 10, 1)  #one hot encode\n",
        "  X_train = data_train[:,1:]\n",
        "  X_train = X_train.reshape(len(X_train), 1, 28, 28)\n",
        "  X_train = X_train/255\n",
        "\n",
        "  data_test = data_array[0:100, :]\n",
        "  Y_test = data_test[:,0]\n",
        "  Y_test = Y_test.reshape(100,1)\n",
        "  X_test = data_test[:,1:]\n",
        "  X_test = X_test.reshape(len(X_test), 1, 28, 28)\n",
        "  X_test = X_test/255\n",
        "\n",
        "  return Y_train, X_train, Y_test, X_test\n",
        "\n",
        "Y_train, X_train, Y_test, X_test = preprocess_data(data)"
      ],
      "metadata": {
        "id": "0tbylD0zmJGx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = [\n",
        "    Convolution((1, 28, 28), 3, 5),\n",
        "    MaxPool2D(),\n",
        "    ReLU(),\n",
        "    Convolution((5, 13, 13), 3, 10),\n",
        "    MaxPool2D(),\n",
        "    ReLU(),\n",
        "    Reshape((10, 6, 6), (10 * 6 * 6, 1)),\n",
        "    Dense(10 * 6 * 6, 128),\n",
        "    ReLU(),\n",
        "    Dense(128, 10),\n",
        "    Softmax()\n",
        "]\n",
        "\n",
        "train(\n",
        "    network,\n",
        "    binary_cross_entropy,\n",
        "    binary_cross_entropy_prime,\n",
        "    X_train,\n",
        "    Y_train,\n",
        "    epochs=10,\n",
        "    learning_rate=0.05\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtxBU_kJUm6K",
        "outputId": "7d22ed99-e82b-4dba-f60f-082151a58d66"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/10, error=0.2548784780950899, accuracy=0.47333333333333333\n",
            "2/10, error=0.1195104015309659, accuracy=0.7533333333333333\n",
            "3/10, error=0.07396714696287807, accuracy=0.8555555555555555\n",
            "4/10, error=0.04917908004492084, accuracy=0.9088888888888889\n",
            "5/10, error=0.0317989665477027, accuracy=0.9488888888888889\n",
            "6/10, error=0.020421381995795136, accuracy=0.9766666666666667\n",
            "7/10, error=0.012158855686555623, accuracy=0.9888888888888889\n",
            "8/10, error=0.007021267323670256, accuracy=0.9966666666666667\n",
            "9/10, error=0.004562241648762289, accuracy=0.9977777777777778\n",
            "10/10, error=0.0031179985857958414, accuracy=1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_predictions = []\n",
        "for x, y in zip(X_test, Y_test):\n",
        "    output = predict(network, x)\n",
        "    pred_number = get_prediction(output)\n",
        "    Y_predictions.append(pred_number)\n",
        "\n",
        "get_accuracy(Y_predictions, Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oWIZ50ZXAup",
        "outputId": "cd4dccf1-9a9b-41f9-f402-cf737dabd065"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting Problem caused maybe because of the lack of convolution padding and number of kernels"
      ],
      "metadata": {
        "id": "CyX7Wrc_0CkW"
      }
    }
  ]
}